{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91286a43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2892/3290035931.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import plotly.express as ex\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from focal_loss import BinaryFocalloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for confusion matrix\n",
    "def confusion_matrix_visualization(model_name, color_seq):\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2, in\n",
    "              zip(group_names, group_percentages)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    ax = sns.heatmap(cm, annot = labels, fmt = '' , cmap = color_seq)\n",
    "\n",
    "    ax.set_title('Confusion Matrix for ' + model_name + ' Classifier')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Actual Values')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"D:/Hack_sert/Downsampled_data.csv\")\n",
    "#df = df.fillna(df.mean())\n",
    "#print(np.shape(df))\n",
    "#print(df.head())\n",
    "# Impute Missing Values with Label Matching Mean\n",
    "# Impute Missing Values with Label Matching Mean\n",
    "for col in ['Sulfate','ph','Trihalomethanes']:\n",
    "    missing_label_0 = df.query('Potability == 0')[col][df[col].isna()].index\n",
    "    df.loc[missing_label_0,col] = df.query('Potability == 0')[col][df[col].notna()].mean()\n",
    "\n",
    "    missing_label_1 = df.query('Potability == 1')[col][df[col].isna()].index\n",
    "    df.loc[missing_label_1,col] = df.query('Potability == 1')[col][df[col].notna()].mean()            \n",
    "                                                                   \n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splititng Train and Test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8c5f9",
   "metadata": {},
   "source": [
    " <h1>Standardization</h1>\n",
    "<p>Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.</p>\n",
    "\n",
    "<p>Here’s the formula for standardization:</p>\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/Stand_eq.gif\">\n",
    "<p>where µ is the mean of the feature values and σ is the standard deviation of the feature values. Note that in this case, the values are not restricted to a particular range.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76bfb4",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression</h2>\n",
    "<p>Logistic regression is a classification algorithm used to assign observations to a discrete set of classes.Logistic regression transforms its output using the logistic sigmoid function to return a probability value</p>\n",
    "<p>Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability.</p>\n",
    "<p>Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.</p>\n",
    "    Logistic regression hypothesis expectation </p>\n",
    "<img src = \"https://miro.medium.com/max/446/1*GnceHPIeThNShGSmYzE4eA.png\">\n",
    "<p>The Hypothesis of logistic regression,</p>\n",
    "<img src = \"https://miro.medium.com/max/1046/1*l59BUnPwWHMf1H-GNxgZHQ.png\" width=\"300\" height=\"100\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cab6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ....Training Logistic Regression model..... \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Logistic Regression Classifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"Logistic Regression Classifier model\\n\")\n",
    "print(\"Confusion Matrix for Logistic Regression Classifier model\\n\", cm)\n",
    "print(\"\\n\\nAccuracy Score for Logistic Regression Classifier model\\n\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07196c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('Logistic Regression', 'Blues')\n",
    "\n",
    "print(\"Train Score Logistic Regression:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score Logistic Reegression:\", classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84597d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa359ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, classifier.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71d4d1",
   "metadata": {},
   "source": [
    "<h3>Decision trees</h3>\n",
    "<p>Decision Tree is a Supervised Machine Learning Algorithm that uses a set of rules to make decisions, similarly to how humans make decisions.Decision trees can perform both classification and regression tasks</p>\n",
    "<p>The intuition behind Decision Trees is that you use the dataset features to create yes/no questions and continually split the dataset until you isolate all data points belonging to each class.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027cefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Decision Tree classifier  model on the training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b32506",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"Decision Tree classifier model\\n\")\n",
    "print(\"Confusion Matrix for Decision Tree classifier model\\n\", cm)\n",
    "print(\"\\n\\nAccuracy Score for Decision Tree classifier model\\n\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('Decision Tree classifier', 'rocket')\n",
    "\n",
    "print(\"Train Score for Decision Tree:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for Decision Tree:\", classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91661447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fb8ce",
   "metadata": {},
   "source": [
    "<h4>k-nearest neighbors (KNN)</h4>\n",
    "<p>The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.</p>\n",
    "<p>The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.</p>\n",
    "<p>The KNN Algorithm\n",
    "1.Load the data<br>\n",
    "2.Initialize K to your chosen number of neighbors<br>\n",
    "3. For each example in the data<br>\n",
    "3.1 Calculate the distance between the query example and the current example from the data.<br>\n",
    "3.2 Add the distance and the index of the example to an ordered collection.<br>\n",
    "4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances<br>\n",
    "5. Pick the first K entries from the sorted collection<br>\n",
    "6. Get the labels of the selected K entries<br>\n",
    "7. If regression, return the mean of the K labels<br>\n",
    "8. If classification, return the mode of the K labels</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ....Training K-NN model.... \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 23, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"K-NN Classifier model\\n\")\n",
    "print(\"Confusion Matrix for K-NN Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for K-NN Classifier model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"K-NN Classifier model\\n\")\n",
    "print(\"Confusion Matrix for K-NN Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for K-NN Classifier model\\n\", accuracy_score(y_test, y_pred)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa929e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('K-NN', 'cubehelix')\n",
    "\n",
    "print(\"Train Score K-NN:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score K-NN:\", classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362faa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2abc3a3",
   "metadata": {},
   "source": [
    "<h5>The Random Forest Classifier</h5>\n",
    "<p>Random forest,consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.</p>\n",
    "<p>A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.</p>\n",
    "<img src =\"https://imgs.search.brave.com/A4isyTogtJ3pON9Cy_-56W2nYjy-iWvctF5CsCJi-Uk/rs:fit:835:225:1/g:ce/aHR0cHM6Ly90c2Uz/Lm1tLmJpbmcubmV0/L3RoP2lkPU9JUC41/OGYxQ1o4TTRpbDBP/WllnMm9STjR3SGFF/TiZwaWQ9QXBp\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .....Training Random Forest classifier  model.....\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Random Forest classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"Random Forest Classifier Model\\n\")\n",
    "print(\"Confusion Matrix for Random Forest classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for Random Forest classifier model\\n\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('Random Forest classifier', 'rocket')\n",
    "\n",
    "print(\"Train Score for Random Forest classifier:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for Random Forest classifier:\", classifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d26bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .....Training Naive Bayes model..... \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Naive Bayes Classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"Naive Bayes Classifier model\\n\")\n",
    "print(\"Confusion Matrix for Naive Bayes Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for Naive Bayes Classifier model\\n\", accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65186bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('Naive Bayes', 'magma')\n",
    "\n",
    "print(\"Train Score for Naive Bayes:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for Naive Bayes:\", classifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .....Training SVM model.....\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for SVM Classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"SVM Classifier model\\n\")\n",
    "print(\"Confusion Matrix for SVM Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for SVM Classifier model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('SVM', 'mako')\n",
    "\n",
    "print(\"Train Score SVM:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score SVM:\", classifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db70a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .....Training Kernel SVM model.....\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Kernel SVM Classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"Kernel SVM Classifier model\\n\")\n",
    "print(\"Confusion Matrix for Kernel SVM Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for kernel SVM Classifier model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57facba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('Kernel SVM', 'Greys')\n",
    "\n",
    "print(\"Train Score for Kernel SVM:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for Kernel SVM:\", classifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a19e3a",
   "metadata": {},
   "source": [
    "<h6>Artificial Neural Networks</h6>\n",
    "<p>Artificial Neural networks (ANN) or neural networks are computational algorithms.\n",
    "It intended to simulate the behavior of biological systems composed of “neurons”. \n",
    "ANNs are computational models inspired by an animal’s central nervous systems. \n",
    "It is capable of machine learning as well as pattern recognition. \n",
    "These presented as systems of interconnected “neurons” which can compute values from inputs.</p>\n",
    "<p>ANN layers</p>\n",
    "<p>Input layer – The activity of the input units represents the raw information that can feed into the network.<br>\n",
    "Hidden layer – To determine the activity of each hidden unit. The activities of the input units and the weights on the connections between the input and the hidden units. There may be one or more hidden layers.<br>\n",
    "Output layer – The behavior of the output units depends on the activity of the hidden units and the weights between the hidden and output units.</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .....Training ANN model.....\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))\n",
    "\n",
    "# Output layer\n",
    "ann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Training ANN\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)\n",
    "\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1),\n",
    "                      y_test.reshape(len(y_test), 1)),\n",
    "                     1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for ANN\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"ANN model\\n\")\n",
    "print(\"Confusion Matrix for ANN model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for ANN model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6242a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('ANN', 'icefire')\n",
    "\n",
    "'''\n",
    "#print(\"Train Score for ANN:\", ann.score(X_train,y_train))\n",
    "print(\"Test Score for ANN:\", ann.ann[\"val_loss\"](X_test,y_test))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = ann, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae623c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ......Training XGBoost Classifier...\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Confusion matrix for XGBoost Classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"XGBoost Classifier model\\n\")\n",
    "print(\"Confusion Matrix for XGBoost Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for XGBoost Classifier model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7529833",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('XGBoost', 'flare')\n",
    "\n",
    "print(\"Train Score for XGBoost:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for XGBoost:\", classifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b579dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#....Training Extra Tree Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "classifier = ExtraTreesClassifier(n_estimators = 5, \n",
    "                           criterion = 'entropy',\n",
    "                           max_features = 2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Extra Tree Classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"Extra Tree Classifier model\\n\")\n",
    "print(\"Confusion Matrix for Extra Tree Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for Extra Tree Classifier model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('Extra Tree', 'rocket')\n",
    "\n",
    "print(\"Train Score for Extra Tree:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for Extra Tree:\", classifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bab169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada Boost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1),\n",
    "                                n_estimators = 200)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for AdaBoost Classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"AdaBoost Classifier model\\n\")\n",
    "print(\"Confusion Matrix for AdaBoost Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for AdaBoost Classifier model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('AdaBoost', 'vlag')\n",
    "\n",
    "print(\"Train Score for AdaBoost:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for AdaBoost:\", classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ea3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier = GradientBoostingClassifier(learning_rate = 0.1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26258fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Gradient Boostingt Classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"Gradient Boosting Classifier model\\n\")\n",
    "print(\"Confusion Matrix for Gradient Boosting Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for Gradient Boosting Classifier model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('Gradient Boosting', 'Spectral')\n",
    "\n",
    "print(\"Train Score for Gradient Boosting:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for Gradient Boosting:\", classifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53417f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light Gradient Boosting Machine Classifier\n",
    "from lightgbm import LGBMClassifier\n",
    "classifier = LGBMClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Light Gradient Boosting Machine Classifier\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"``````````````````````````````````````````````````````\\n\")\n",
    "print(\"Light Gradient Boosting Machine Classifier model\\n\")\n",
    "print(\"Confusion Matrix for Light Gradient Boosting Machine Classifier model\\n\",cm)\n",
    "print(\"\\n\\nAccuracy Score for Light Gradient Boosting Machine Classifier model\\n\", accuracy_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64956c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_visualization('Light Gradient Boosting Machine', 'mako')\n",
    "\n",
    "print(\"Train Score for Light Gradient Boosting Machine:\", classifier.score(X_train,y_train))\n",
    "print(\"Test Score for Light Gradient Boosting Machine:\", classifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ec677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"\\nK-Fold Cross Validation\\nAccuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6979c69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
